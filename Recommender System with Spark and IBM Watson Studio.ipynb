{
    "nbformat_minor": 2, 
    "cells": [
        {
            "source": "# Recommender System with Spark and IBM Watson Studio", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "When your website generates a lot of explicit data e.g. in the form of ratings, it makes a great starting point for making user experience more personalized. <br>\nIn this example, we'll follow the collaborative filtering approach and apply Alternating Least Squares (ALS) algorithm to create a recommedation engine for a website.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from pyspark import SparkContext, SQLContext\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession\\\n  .builder\\\n  .appName(\"Recommender system in Spark\")\\\n  .config(\"spark.some.config.option\", \"some-value\") \\\n  .getOrCreate()", 
            "cell_type": "code", 
            "execution_count": 1, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20190816092651-0003\nKERNEL_ID = 1d774b1b-3578-4c7a-a81f-b38ab48e200f\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "Let's look into the MovieLens datasets we are going to use here.<br>\nThere are two csv files we need to merge based on 'movieId' column:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import ibmos2spark\ncredentials = {\n    'endpoint': 'https://s3.eu-geo.objectstorage.service.networklayer.com',\n    'service_id': 'iam-ServiceId-9ebf6bf4-20a4-47da-bd08-ba8e7807ae90',\n    'iam_service_endpoint': 'https://iam.eu-gb.bluemix.net/oidc/token',\n    'api_key': 'PC5PWGuwGlO3Ch7Ygh3Fc4CU_zACtcXN1z7Yf3zNFjA7'\n}\n\nconfiguration_name = 'os_3f4c7db4c46d4481b29169bb36b6aa49_configs'\ncos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\nmovies = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load(cos.url('movies.csv', 'recommendationengine-donotdelete-pr-1cnfjaobjuzd3x'))\nmovies.show(5)", 
            "cell_type": "code", 
            "execution_count": 2, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-------+--------------------+--------------------+\n|movieId|               title|              genres|\n+-------+--------------------+--------------------+\n|      1|    Toy Story (1995)|Adventure|Animati...|\n|      2|      Jumanji (1995)|Adventure|Childre...|\n|      3|Grumpier Old Men ...|      Comedy|Romance|\n|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n|      5|Father of the Bri...|              Comedy|\n+-------+--------------------+--------------------+\nonly showing top 5 rows\n\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "ratings = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load(cos.url('ratings.csv', 'recommendationengine-donotdelete-pr-1cnfjaobjuzd3x'))\nratings.show(5)", 
            "cell_type": "code", 
            "execution_count": 3, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+------+-------+------+----------+\n|userId|movieId|rating| timestamp|\n+------+-------+------+----------+\n|     1|      2|   3.5|1112486027|\n|     1|     29|   3.5|1112484676|\n|     1|     32|   3.5|1112484819|\n|     1|     47|   3.5|1112484727|\n|     1|     50|   3.5|1112484580|\n+------+-------+------+----------+\nonly showing top 5 rows\n\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "ratings.join(movies, 'movieId').show(3)", 
            "cell_type": "code", 
            "execution_count": 4, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-------+------+------+----------+--------------------+--------------------+\n|movieId|userId|rating| timestamp|               title|              genres|\n+-------+------+------+----------+--------------------+--------------------+\n|      2|     1|   3.5|1112486027|      Jumanji (1995)|Adventure|Childre...|\n|     29|     1|   3.5|1112484676|City of Lost Chil...|Adventure|Drama|F...|\n|     32|     1|   3.5|1112484819|Twelve Monkeys (a...|Mystery|Sci-Fi|Th...|\n+-------+------+------+----------+--------------------+--------------------+\nonly showing top 3 rows\n\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "We shall now check if there are missing values in the dataset and convert data type of the variables into integers for further analysis: ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "df = ratings.select('userId', 'movieId', 'rating')\ndf.dtypes", 
            "cell_type": "code", 
            "execution_count": 5, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[('userId', 'string'), ('movieId', 'string'), ('rating', 'string')]"
                    }, 
                    "execution_count": 5
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "from pyspark.sql.functions import isnan\n\nprint(df.filter((df['userId'] == \"\") | df['userId'].isNull() | isnan(df['userId'])).count())\nprint(df.filter((df['movieId'] == \"\") | df['movieId'].isNull() | isnan(df['movieId'])).count())\nprint(df.filter((df['rating'] == \"\") | df['rating'].isNull() | isnan(df['rating'])).count())", 
            "cell_type": "code", 
            "execution_count": 6, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "0\n0\n0\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "from pyspark.sql.types import IntegerType\n\ndf = df.withColumn('userId', df['userId'].cast(IntegerType()))\ndf = df.withColumn('movieId', df['movieId'].cast(IntegerType()))\ndf = df.withColumn('rating', df['rating'].cast(IntegerType()))", 
            "cell_type": "code", 
            "execution_count": 7, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "df.dtypes", 
            "cell_type": "code", 
            "execution_count": 8, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[('userId', 'int'), ('movieId', 'int'), ('rating', 'int')]"
                    }, 
                    "execution_count": 8
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "Let's now create subsets of data for training and testing the model:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "train_test_splits = df.randomSplit([0.8, 0.2])\ntrain = train_test_splits[0].withColumnRenamed('rating', 'label')\ntest = train_test_splits[1].withColumnRenamed('rating', 'trueRating')", 
            "cell_type": "code", 
            "execution_count": 9, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "We'll apply Alternating Least Squares method to train and test the model: ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from pyspark.ml.recommendation import ALS\n\nals = ALS(maxIter=19, regParam=0.01, userCol='userId', itemCol='movieId', ratingCol='label')\n\nmodel = als.fit(train)\nprediction = model.transform(test)", 
            "cell_type": "code", 
            "execution_count": 10, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "Let's take a look at some of the predictions the model made, compare them to the true values and compute RMSE:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "prediction.join(movies, 'movieId').select('userId', 'title', 'prediction', 'trueRating').show(n=10, truncate=False)", 
            "cell_type": "code", 
            "execution_count": 11, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+------+--------------------------------+----------+----------+\n|userId|title                           |prediction|trueRating|\n+------+--------------------------------+----------+----------+\n|3673  |Awfully Big Adventure, An (1995)|2.3890615 |2         |\n|5186  |Awfully Big Adventure, An (1995)|3.4688776 |2         |\n|903   |Awfully Big Adventure, An (1995)|1.2790377 |3         |\n|3335  |Awfully Big Adventure, An (1995)|2.2354188 |5         |\n|3673  |Guilty as Sin (1993)            |3.3912914 |3         |\n|2242  |Guilty as Sin (1993)            |2.434418  |3         |\n|4162  |Guilty as Sin (1993)            |2.713019  |3         |\n|51    |Guilty as Sin (1993)            |2.0280638 |2         |\n|2274  |Guilty as Sin (1993)            |3.0363755 |3         |\n|3352  |Hudsucker Proxy, The (1994)     |3.1329057 |3         |\n+------+--------------------------------+----------+----------+\nonly showing top 10 rows\n\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "from pyspark.ml.evaluation import RegressionEvaluator\n\nevaluator = RegressionEvaluator(labelCol='trueRating', predictionCol='prediction', metricName='rmse')\nprediction = prediction.dropna(how='any', subset=[\"prediction\"])\nrmse = evaluator.evaluate(prediction)\n\nprint ('Root Mean Square Error:', rmse)", 
            "cell_type": "code", 
            "execution_count": 12, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Root Mean Square Error: 0.9252403069943134\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "Finally we are in a position to generate recommendations for all the users or pick a user to see what the recemmendation system we've build is recommending specifically to them:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "userRecs = model.recommendForAllUsers(10)\nuserRecs.show(5)", 
            "cell_type": "code", 
            "execution_count": 17, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+------+--------------------+\n|userId|     recommendations|\n+------+--------------------+\n|   471|[[25963, 7.063578...|\n|  1591|[[53161, 9.807543...|\n|  4101|[[59549, 6.138625...|\n|  3794|[[7930, 7.566674]...|\n|  6654|[[27888, 8.70235]...|\n+------+--------------------+\nonly showing top 5 rows\n\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "This user defined function I came across on the Internet makes it easy to make recommendations for a specific user:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "def get_recs_for_users(recs):\n    recs = recs.select('recommendations.movieId','recommendations.rating')\n    movies = recs.select('movieId').toPandas().iloc[0,0]\n    ratings = recs.select('rating').toPandas().iloc[0,0]\n    ratings_matrix = pd.DataFrame(movies,columns = ['movieId'])\n    ratings_matrix['ratings'] = ratings\n    ratings_matrix_ps = sqlContext.createDataFrame(ratings_matrix)\n    return ratings_matrix_ps", 
            "cell_type": "code", 
            "execution_count": 18, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "from pyspark.sql.functions import col \nimport pandas as pd\nuser_1000_recs = userRecs.filter(col('userId') == 1000) \nget_recs_for_users(user_1000_recs).show(10)", 
            "cell_type": "code", 
            "execution_count": 21, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-------+------------------+\n|movieId|           ratings|\n+-------+------------------+\n|   1163|10.180137634277344|\n|   2079| 8.391616821289062|\n|  66200| 8.203961372375488|\n|  48883| 8.014423370361328|\n|  59727|7.9298248291015625|\n|   5840|  7.76695442199707|\n| 102684|7.7248735427856445|\n|  50158| 7.722866058349609|\n|   5461| 7.702124118804932|\n|  64114| 7.623742580413818|\n+-------+------------------+\n\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "Well done! Now we have a recommendation system to offer the user more personalized experience.", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "nbformat": 4, 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6 with Spark", 
            "name": "python36", 
            "language": "python3"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.6.8", 
            "name": "python", 
            "pygments_lexer": "ipython3", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }
}