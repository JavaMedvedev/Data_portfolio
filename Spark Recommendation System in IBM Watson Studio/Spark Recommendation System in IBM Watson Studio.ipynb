{
    "nbformat_minor": 2, 
    "cells": [
        {
            "source": "from pyspark import SparkContext, SQLContext", 
            "cell_type": "code", 
            "execution_count": 1, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20190815131323-0000\nKERNEL_ID = 49ed001b-5c9d-409c-8f75-e7ec0e974b14\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator", 
            "cell_type": "code", 
            "execution_count": 2, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "from pyspark.ml.evaluation import RegressionEvaluator", 
            "cell_type": "code", 
            "execution_count": 3, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "#import module\nfrom pyspark.ml.recommendation import ALS\n\n#create session\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession\\\n  .builder\\\n  .appName(\"Recommender system in Spark\")\\\n  .config(\"spark.some.config.option\", \"some-value\") \\\n  .getOrCreate()", 
            "cell_type": "code", 
            "execution_count": 4, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "import ibmos2spark\n# @hidden_cell\ncredentials = {\n    'endpoint': 'https://s3.eu-geo.objectstorage.service.networklayer.com',\n    'service_id': 'iam-ServiceId-9ebf6bf4-20a4-47da-bd08-ba8e7807ae90',\n    'iam_service_endpoint': 'https://iam.eu-gb.bluemix.net/oidc/token',\n    'api_key': 'PC5PWGuwGlO3Ch7Ygh3Fc4CU_zACtcXN1z7Yf3zNFjA7'\n}\n\nconfiguration_name = 'os_3f4c7db4c46d4481b29169bb36b6aa49_configs'\ncos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\nmovies = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load(cos.url('movies.csv', 'recommendationengine-donotdelete-pr-1cnfjaobjuzd3x'))\nmovies.take(5)", 
            "cell_type": "code", 
            "execution_count": 35, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[Row(movieId='1', title='Toy Story (1995)', genres='Adventure|Animation|Children|Comedy|Fantasy'),\n Row(movieId='2', title='Jumanji (1995)', genres='Adventure|Children|Fantasy'),\n Row(movieId='3', title='Grumpier Old Men (1995)', genres='Comedy|Romance'),\n Row(movieId='4', title='Waiting to Exhale (1995)', genres='Comedy|Drama|Romance'),\n Row(movieId='5', title='Father of the Bride Part II (1995)', genres='Comedy')]"
                    }, 
                    "execution_count": 35
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "ratings = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load(cos.url('ratings.csv', 'recommendationengine-donotdelete-pr-1cnfjaobjuzd3x'))\nratings.take(5)", 
            "cell_type": "code", 
            "execution_count": 36, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[Row(userId='1', movieId='2', rating='3.5', timestamp='1112486027'),\n Row(userId='1', movieId='29', rating='3.5', timestamp='1112484676'),\n Row(userId='1', movieId='32', rating='3.5', timestamp='1112484819'),\n Row(userId='1', movieId='47', rating='3.5', timestamp='1112484727'),\n Row(userId='1', movieId='50', rating='3.5', timestamp='1112484580')]"
                    }, 
                    "execution_count": 36
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "#merge \"movies\" and \"ratings\" dataFrame based on \"movieId\"\nratings.join(movies, \"movieId\").show(3)", 
            "cell_type": "code", 
            "execution_count": 37, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-------+------+------+----------+--------------------+--------------------+\n|movieId|userId|rating| timestamp|               title|              genres|\n+-------+------+------+----------+--------------------+--------------------+\n|      2|     1|   3.5|1112486027|      Jumanji (1995)|Adventure|Childre...|\n|     29|     1|   3.5|1112484676|City of Lost Chil...|Adventure|Drama|F...|\n|     32|     1|   3.5|1112484819|Twelve Monkeys (a...|Mystery|Sci-Fi|Th...|\n+-------+------+------+----------+--------------------+--------------------+\nonly showing top 3 rows\n\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "ratings.dtypes", 
            "cell_type": "code", 
            "execution_count": 38, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[('userId', 'string'),\n ('movieId', 'string'),\n ('rating', 'string'),\n ('timestamp', 'string')]"
                    }, 
                    "execution_count": 38
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "#use only column data of \"userId\", \"movieId\", dan \"rating\"\ndata = ratings.select(\"userId\", \"movieId\", \"rating\")", 
            "cell_type": "code", 
            "execution_count": 40, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "from pyspark.sql.types import IntegerType\ndata = data.withColumn(\"userId\", data[\"userId\"].cast(IntegerType()))\ndata = data.withColumn(\"movieId\", data[\"movieId\"].cast(IntegerType()))\ndata = data.withColumn(\"rating\", data[\"rating\"].cast(IntegerType()))", 
            "cell_type": "code", 
            "execution_count": 41, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "#data['userId'] = data['userId'].astype(int)\n#data['movieId'] = data['movieId'].astype(int)\n#data['rating'] = data['rating'].astype(int)", 
            "cell_type": "code", 
            "execution_count": 42, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "data.dtypes", 
            "cell_type": "code", 
            "execution_count": 43, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[('userId', 'int'), ('movieId', 'int'), ('rating', 'int')]"
                    }, 
                    "execution_count": 43
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "#divide data, 70% for training and 30% for testing\nsplits = data.randomSplit([0.7, 0.3])\ntrain = splits[0].withColumnRenamed(\"rating\", \"label\")\ntest = splits[1].withColumnRenamed(\"rating\", \"trueLabel\")\n#calculate number of rows\ntrain_rows = train.count()\ntest_rows = test.count()\nprint (\"number of training data rows:\", train_rows, \n       \", number of testing data rows:\", test_rows)", 
            "cell_type": "code", 
            "execution_count": 44, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "number of training data rows: 734252 , number of testing data rows: 314323\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "#define ALS (Alternating Least Square) as our recommender system\nals = ALS(maxIter=19, regParam=0.01, userCol=\"userId\", \n          itemCol=\"movieId\", ratingCol=\"label\")\n#train our ALS model\nmodel = als.fit(train)\nprint(\"Training is done!\")", 
            "cell_type": "code", 
            "execution_count": 45, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Training is done!\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "prediction = model.transform(test)\nprint(\"testing is done!\")", 
            "cell_type": "code", 
            "execution_count": 46, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "testing is done!\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "prediction.join(movies, \"movieId\").select(\n    \"userId\", \"title\", \"prediction\", \"trueLabel\").show(n=10, truncate=False)", 
            "cell_type": "code", 
            "execution_count": 47, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+------+--------------------------------+----------+---------+\n|userId|title                           |prediction|trueLabel|\n+------+--------------------------------+----------+---------+\n|6225  |Awfully Big Adventure, An (1995)|1.0461599 |2        |\n|1259  |Awfully Big Adventure, An (1995)|4.4552374 |5        |\n|5814  |Awfully Big Adventure, An (1995)|2.1236084 |3        |\n|4162  |Guilty as Sin (1993)            |2.3960946 |3        |\n|4324  |Guilty as Sin (1993)            |3.885243  |4        |\n|4986  |Guilty as Sin (1993)            |3.5348058 |4        |\n|2242  |Guilty as Sin (1993)            |2.2652295 |3        |\n|156   |Guilty as Sin (1993)            |3.590667  |4        |\n|5518  |Hudsucker Proxy, The (1994)     |3.5980818 |4        |\n|3352  |Hudsucker Proxy, The (1994)     |3.2893536 |3        |\n+------+--------------------------------+----------+---------+\nonly showing top 10 rows\n\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "#import RegressionEvaluator since we also want to calculate RMSE (Root Mean Square Error)\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nevaluator = RegressionEvaluator(\n    labelCol=\"trueLabel\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(prediction)\nprint (\"Root Mean Square Error (RMSE):\", rmse)", 
            "cell_type": "code", 
            "execution_count": 48, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Root Mean Square Error (RMSE): nan\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "prediction.count()\na = prediction.count()\nprint(\"number of original data rows: \", a)\n#drop rows with any missing data\ncleanPred = prediction.dropna(how=\"any\", subset=[\"prediction\"])\nb = cleanPred.count()\nprint(\"number of rows after dropping data with missing value: \", b)\nprint(\"number of missing data: \", a-b)", 
            "cell_type": "code", 
            "execution_count": 49, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "number of original data rows:  314323\nnumber of rows after dropping data with missing value:  313259\nnumber of missing data:  1064\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "rmse = evaluator.evaluate(cleanPred)\nprint (\"Root Mean Square Error (RMSE):\", rmse)", 
            "cell_type": "code", 
            "execution_count": 50, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Root Mean Square Error (RMSE): 0.9464697990256984\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "#Generate recommendations for all the users\nuserRecs = model.recommendForAllUsers(10).show(5)", 
            "cell_type": "code", 
            "execution_count": 65, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+------+--------------------+\n|userId|     recommendations|\n+------+--------------------+\n|  1580|[[1477, 15.164588...|\n|  4900|[[94985, 10.14870...|\n|  5300|[[59549, 10.24481...|\n|  6620|[[757, 8.254283],...|\n|   463|[[59549, 9.028948...|\n+------+--------------------+\nonly showing top 5 rows\n\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "def get_recs_for_users(recs):\n    #Recs should be for a specific user\n    recs = recs.select('recommendations.movieId','recommendations.rating')\n    movies = recs.select('movieId').toPandas().iloc[0,0]\n    ratings = recs.select('rating').toPandas().iloc[0,0]\n    ratings_matrix = pd.DataFrame(movies,columns = ['movieId'])\n    ratings_matrix['ratings'] = ratings\n    ratings_matrix_ps = sqlContext.createDataFrame(ratings_matrix)\n    return ratings_matrix_ps", 
            "cell_type": "code", 
            "execution_count": 58, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "#Recommendations for user 41\n\nfrom pyspark.sql.functions import col \nuser_41_recs = user_recs.filter(col(\"userId\") == 41) \nget_recs_for_users(user_41_recs).head(16)", 
            "cell_type": "code", 
            "execution_count": 59, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[Row(movieId=6234, ratings=9.639665603637695),\n Row(movieId=56167, ratings=9.2036714553833),\n Row(movieId=32456, ratings=8.928771018981934),\n Row(movieId=79318, ratings=8.317727088928223),\n Row(movieId=7319, ratings=7.845831394195557),\n Row(movieId=7222, ratings=7.8397932052612305),\n Row(movieId=7243, ratings=7.720480918884277),\n Row(movieId=8235, ratings=7.519848823547363),\n Row(movieId=2079, ratings=7.4928741455078125),\n Row(movieId=3289, ratings=7.429114818572998)]"
                    }, 
                    "execution_count": 59
                }
            ], 
            "metadata": {}
        }
    ], 
    "nbformat": 4, 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6 with Spark", 
            "name": "python36", 
            "language": "python3"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.6.8", 
            "name": "python", 
            "pygments_lexer": "ipython3", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }
}